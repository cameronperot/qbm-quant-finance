{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444cdef5-48f1-49a6-9b96-9b63a8ab2663",
   "metadata": {},
   "source": [
    "# QBM: Log Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8e8b6a-56bd-4881-ab78-72322c441ecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18914/1026657628.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# load anneal schedule data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m df_anneal = pd.read_csv(\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mproject_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;34m/\u001b[0m \u001b[0;34m\"data/anneal_schedules/csv/09-1265A-A_Advantage_system5_1_annealing_schedule.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_dir' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 s (started: 2022-02-08 16:33:50 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime\n",
    "\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dwave.system import DWaveSampler, FixedEmbeddingComposite\n",
    "from minorminer import find_embedding\n",
    "from numba import njit\n",
    "from scipy.constants import k as k_B, h as h_P\n",
    "\n",
    "k_B /= h_P * 1e9\n",
    "\n",
    "from qbm.models import BQRBM\n",
    "from qbm.plotting import plot_qq\n",
    "from qbm.utils import (\n",
    "    binarize_df,\n",
    "    convert_bin_list_to_str,\n",
    "    get_binarization_params,\n",
    "    get_project_dir,\n",
    "    get_rng,\n",
    "    kl_divergence,\n",
    "    load_artifact,\n",
    "    load_log_returns,\n",
    "    lr_exp_decay,\n",
    "    prepare_training_data,\n",
    "    save_artifact,\n",
    "    unbinarize_df,\n",
    "    compute_stats_over_dfs\n",
    ")\n",
    "\n",
    "# load anneal schedule data\n",
    "df_anneal = pd.read_csv(\n",
    "    project_dir\n",
    "    / \"data/anneal_schedules/csv/09-1265A-A_Advantage_system5_1_annealing_schedule.csv\",\n",
    "    index_col=\"s\",\n",
    ")\n",
    "if 0.5 not in df_anneal.index:\n",
    "    df_anneal.loc[0.5] = (df_anneal.loc[0.499] + df_anneal.loc[0.501]) / 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fdfc1-7d3c-41f9-b789-d89c818172ec",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29756b-778f-41ea-abce-cc876b1490b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "model_name = \"baseline\"\n",
    "model_id = \"01\"\n",
    "\n",
    "project_dir = get_project_dir()\n",
    "data_dir = project_dir / \"data\"\n",
    "models_dir = project_dir / \"artifacts/qbm/log_returns/models\"\n",
    "if not models_dir.exists():\n",
    "    models_dir.mkdir(parents=True)\n",
    "plots_dir = project_dir / \"results/plots/qbm/log_returns\"\n",
    "if not plots_dir.exists():\n",
    "    plots_dir.mkdir(parents=True)\n",
    "config_path = models_dir / f\"{model_name}/config.json\"\n",
    "config = load_artifact(config_path)\n",
    "\n",
    "model_params = config[\"model\"]\n",
    "data_params = config[\"data\"]\n",
    "if model_id is None:\n",
    "    model_params[\"id\"] = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "else:\n",
    "    model_params[\"id\"] = model_id\n",
    "artifacts_dir = models_dir / model_name / model_params[\"id\"]\n",
    "if not artifacts_dir.exists():\n",
    "    artifacts_dir.mkdir(parents=True)\n",
    "config[\"model\"][\"id\"] = model_params[\"id\"]\n",
    "\n",
    "rng = get_rng(model_params[\"seed\"])\n",
    "\n",
    "# data loading\n",
    "date_format = \"%Y-%m-%d\"\n",
    "start_date = datetime.strptime(data_params[\"start_date\"], date_format)\n",
    "end_date = datetime.strptime(data_params[\"end_date\"], date_format)\n",
    "if model_params[\"volatility_indicators\"]:\n",
    "    start_date -= timedelta(days=90)\n",
    "\n",
    "log_returns = load_log_returns(\n",
    "    data_params[\"data_source\"],\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    outlier_threshold=data_params[\"outlier_threshold\"],\n",
    ")\n",
    "log_returns_raw = log_returns.copy()\n",
    "\n",
    "# volatility indicators\n",
    "volatility_binarized = None\n",
    "if model_params[\"volatility_indicators\"]:\n",
    "    volatility_binarized = binarize_volatility(\n",
    "        compute_rolling_volatility(log_returns, timedelta(days=90))\n",
    "    )\n",
    "\n",
    "# data transformation\n",
    "transformer = None\n",
    "if model_params[\"transform\"].get(\"type\") is not None:\n",
    "    if model_params[\"transform\"][\"type\"] == \"quantile\":\n",
    "        transformer = QuantileTransformer(**model_params[\"transform\"][\"params\"])\n",
    "        log_returns = pd.DataFrame(\n",
    "            transformer.fit_transform(log_returns),\n",
    "            columns=log_returns.columns,\n",
    "            index=log_returns.index,\n",
    "        )\n",
    "    elif model_params[\"transform\"][\"type\"] == \"power\":\n",
    "        transformer = PowerTransformer(\n",
    "            log_returns, **model_params[\"transform\"][\"params\"]\n",
    "        )\n",
    "        log_returns = transformer.transform(log_returns)\n",
    "\n",
    "# binarization\n",
    "binarization_params = get_binarization_params(\n",
    "    log_returns, n_bits=model_params[\"n_bits\"]\n",
    ")\n",
    "log_returns_binarized = binarize_df(log_returns, binarization_params)\n",
    "model_params[\"binarization_params\"] = binarization_params\n",
    "\n",
    "# create the training set\n",
    "training_data = prepare_training_data(log_returns_binarized, volatility_binarized)\n",
    "X_train = training_data[\"X_train\"]\n",
    "rng.shuffle(X_train)\n",
    "model_params[\"X_train_shape\"] = X_train.shape\n",
    "model_params[\"columns\"] = training_data[\"columns\"]\n",
    "model_params[\"split_indices\"] = training_data[\"split_indices\"]\n",
    "\n",
    "# save the config\n",
    "model_params[\"n_visible\"] = X_train.shape[-1]\n",
    "model_params[\"n_qubits\"] = model_params[\"n_visible\"] + model_params[\"n_hidden\"]\n",
    "save_artifact(config, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde6533a-bff5-4559-beb4-8850f548d082",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542e81e-a4c6-4114-b896-15dd05e20e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings = False\n",
    "max_chain_length = 7\n",
    "max_qubits = 400\n",
    "embedding_ids = range(1, 11)\n",
    "embeddings_dir = (\n",
    "    project_dir\n",
    "    / f\"artifacts/qbm/log_returns/embeddings/{model_params['n_visible']}x{model_params['n_hidden']}\"\n",
    ")\n",
    "embeddings = {}\n",
    "if generate_embeddings:\n",
    "    # generate the underlying graphical structure to use for determining the embedding\n",
    "    qpu = DWaveSampler(**model_params[\"qpu\"])\n",
    "    source_edgelist = []\n",
    "    for i in range(model_params[\"n_visible\"]):\n",
    "        for j in range(model_params[\"n_visible\"], model_params[\"n_qubits\"]):\n",
    "            source_edgelist.append((i, j))\n",
    "    _, target_edgelist, target_adjacency = qpu.structure\n",
    "\n",
    "    # generate embeddings which satisfy the max chain length\n",
    "    for embedding_id in embedding_ids:\n",
    "        max_chain_length_satisfied = False\n",
    "        while not max_chain_length_satisfied:\n",
    "            # generate embedding\n",
    "            embedding = find_embedding(source_edgelist, target_edgelist)\n",
    "\n",
    "            # check max chain length\n",
    "            for logical_qubit, physical_qubits in embedding.items():\n",
    "                if len(physical_qubits) > max_chain_length:\n",
    "                    break\n",
    "            else:\n",
    "                if np.sum([len(x) for x in embedding.values()]) <= max_qubits:\n",
    "                    max_chain_length_satisfied = True\n",
    "\n",
    "        embeddings[embedding_id] = embedding\n",
    "        \n",
    "    # save embeddings\n",
    "    for embedding_id, embedding in embeddings.items():\n",
    "        save_artifact(embeddings_dir / f\"{embedding_id:02}.json\")\n",
    "else:\n",
    "    for embedding_path in sorted([x for x in embeddings_dir.iterdir()]):\n",
    "        embedding_id = int(embedding_path.stem)\n",
    "        embeddings[embedding_id] = {\n",
    "            int(k): v for k, v in load_artifact(embedding_path).items()\n",
    "        }\n",
    "\n",
    "chain_lengths = {}\n",
    "for embedding_id, embedding in embeddings.items():\n",
    "    chain_lengths[embedding_id] = {i: 0 for i in range(1, max_chain_length + 1)}\n",
    "    for logical_qubit, physical_qubits in embedding.items():\n",
    "        chain_length = len(physical_qubits)\n",
    "        chain_lengths[embedding_id][chain_length] += 1\n",
    "chain_lengths = pd.DataFrame.from_dict(chain_lengths, orient=\"index\")\n",
    "chain_lengths[\"n_qubits\"] = (chain_lengths * np.arange(1, max_chain_length + 1)).sum(axis=1)\n",
    "chain_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa23263-9e01-427d-bda1-605d511ef831",
   "metadata": {},
   "source": [
    "## Embedding 1 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd9480e-84f5-4076-859b-5a6091f730b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (11.0, 0.55), (11.225, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.94 ms (started: 2022-02-08 16:28:15 +01:00)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e723d8-2f37-48e7-944a-c8abf563c02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "time: 2.59 ms (started: 2022-02-08 16:29:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# set the model params\n",
    "s_freeze = 1.0\n",
    "train_model = True\n",
    "embedding_ids = [1]\n",
    "n_epochs = 50\n",
    "\n",
    "# set anneal schedule\n",
    "t_a = 20\n",
    "s_pause = 0.55\n",
    "\u03b1_quench = 2\n",
    "t_pause = round(s_pause * t_a, 3)\n",
    "\u0394_quench = round((1 - s_pause) / \u03b1_quench, 3)\n",
    "if s_pause == 1:\n",
    "    anneal_schedule = [(0, 0), (t_pause, s_pause)]\n",
    "else:\n",
    "    anneal_schedule = [\n",
    "        (0, 0),\n",
    "        (t_pause, s_pause),\n",
    "        (round(t_pause + \u0394_quench, 3), 1),\n",
    "    ]\n",
    "\n",
    "# train the models\n",
    "models_annealer = {}\n",
    "for relative_chain_strength in np.arange(1, 11) / 10:\n",
    "    # set model name and path\n",
    "    model_name = f\"model_annealer-s_pause={s_pause:.2f}-s={s:.2f}-embedding_{embedding_id:02}\"\n",
    "    model_path = artifact_dir / f\"models/embedding_01_rcs/{model_name}.pkl\"\n",
    "    if train_model:\n",
    "        # model_annealer params\n",
    "        beta_initial = 0.45\n",
    "        exact_params = None\n",
    "\n",
    "        # training params\n",
    "        n_epochs = 100\n",
    "        n_samples = 10_000\n",
    "        learning_rate = 0.1\n",
    "        mini_batch_size = 10\n",
    "        epochs = np.arange(1, n_epochs + 1)\n",
    "        learning_rates = learning_rate * lr_exp_decay(\n",
    "            epochs, decay_epoch=50, period=10\n",
    "        )\n",
    "        learning_rates_beta = learning_rate * lr_exp_decay(\n",
    "            epochs, decay_epoch=50, period=20\n",
    "        )\n",
    "\n",
    "        # set the anneal params\n",
    "        anneal_params = {\n",
    "            \"s\": s_freeze,\n",
    "            \"A\": df_anneal.loc[s_freeze, \"A(s) (GHz)\"],\n",
    "            \"B\": df_anneal.loc[s_freeze, \"B(s) (GHz)\"],\n",
    "            \"schedule\": anneal_schedule,\n",
    "        }\n",
    "\n",
    "        # skip if model already exists\n",
    "        if model_path.exists():\n",
    "            print(\"Model already exists\")\n",
    "            continue\n",
    "\n",
    "        # model init\n",
    "        model_annealer = BQRBM(\n",
    "            X_train=X_train,\n",
    "            n_hidden=n_hidden,\n",
    "            embedding=embedding,\n",
    "            anneal_params=anneal_params,\n",
    "            beta_initial=beta_initial,\n",
    "            exact_params=exact_params,\n",
    "        )\n",
    "\n",
    "        # model train and save\n",
    "        model_annealer.train(\n",
    "            n_epochs=n_epochs,\n",
    "            n_samples=n_samples,\n",
    "            learning_rate=learning_rates,\n",
    "            learning_rate_beta=learning_rates_beta,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            callback=callback,\n",
    "        )\n",
    "        models_annealer[embedding_id, s_pause] = model_annealer\n",
    "        model_annealer.save(model_path)\n",
    "\n",
    "        # save attributes as dict in case of error loading old pickled object\n",
    "        model_annealer_attributes = {\n",
    "            \"A\": model_annealer.A,\n",
    "            \"B\": model_annealer.B,\n",
    "            \"a\": model_annealer.a,\n",
    "            \"b\": model_annealer.b,\n",
    "            \"W\": model_annealer.W,\n",
    "            \"beta\": model_annealer.beta,\n",
    "            \"embedding\": model_annealer.embedding,\n",
    "            \"qpu_params\": model_annealer.qpu_params,\n",
    "            \"anneal_params\": model_annealer.anneal_params,\n",
    "            \"exact_params\": model_annealer.exact_params,\n",
    "            \"beta_history\": model_annealer.beta_history,\n",
    "            \"callback_outputs\": [x for x in model_annealer.callback_outputs],\n",
    "        }\n",
    "        save_artifact(\n",
    "            model_annealer_attributes,\n",
    "            artifact_dir / f\"models/{model_name}-attributes.pkl\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96836bc4-c110-40aa-aac9-b6a9f9e5bb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}