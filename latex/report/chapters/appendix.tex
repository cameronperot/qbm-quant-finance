\section{RBM}

\subsection{Conditional Probabilities}\label{app:conditional_probabilities_derivation}
We start by noting
\begin{align}
    p(\vec{v}) = \frac{1}{Z} \sum_\vec{h} e^{-E(\vec{v},\vec{h})}
\end{align}
From this we can derive the conditional probability
\begin{align}
    p(\vec{h} | \vec{v})
        &= \frac{p(\vec{v},\vec{h})}{p(\vec{v})} \\
        &= \frac{1}{p(\vec{v})} \frac{1}{Z} \exp( \vec{a}^\intercal\vec{v} + \vec{b}^\intercal\vec{h} + \vec{v}^\intercal\mat{W}\vec{h} ) \\
        &= \frac{1}{Z'} \exp\bigg( \sum_j b_j h_j + \sum_j (\vec{v}^\intercal\mat{W})_j h_j \bigg) \\
        &= \frac{1}{Z'} \prod_j \exp\big( b_j h_j + (\vec{v}^\intercal\mat{W})_j h_j \big)
\end{align}
where
\begin{align}
    Z' = \sum_\vec{h} \exp( \vec{b}^\intercal\vec{h} + \vec{v}^\intercal\mat{W}\vec{h} )
\end{align}
This leads us to
\begin{align}
    p(h_j = 1 | \vec{v})
        &= \frac{\tilde{p}(h_j = 1 | \vec{v})}{\tilde{p}(h_j = 0 | \vec{v}) + \tilde{p}(h_j = 1 | \vec{v})} \\
        &= \frac{\exp\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)}{1 + \exp\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)} \\
        &= \sigma\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)
\end{align}
Finally, we have
\begin{align}
    p(\vec{h} | \vec{v}) = \prod_j \sigma\big( (2\vec{h} - 1) \odot (\vec{b} + \mat{W}^\intercal\vec{v}) \big)_j
\end{align}

Analogously for \( p(\vec{v} | \vec{h}) \) one finds
\begin{align}
    p(\vec{v} | \vec{h}) = \prod_i \sigma\big( (2\vec{v} - 1) \odot (\vec{a} + \mat{W}\vec{h}) \big)_i
\end{align}

\subsection{Log-Likelihood}\label{app:rbm_log_likelihood_derivation}
For data distribution \( p_\text{data} \) and parameters \( \theta = (\mat{W}, \vec{a}, \vec{b}) \) the average log-likelihood is given by
\begin{align}
    \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log p(\vec{v}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \sum_\vec{h} p(\vec{v},\vec{h}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \frac{1}{Z} \sum_\vec{h} e^{-E(\vec{v},\vec{h})}  \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \sum_\vec{h} e^{-E(\vec{v},\vec{h})} - \log \sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})}
\end{align}
Taking the gradient we find
\begin{align}
    \partial_{\theta} \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \frac{\sum_\vec{h} e^{-E(\vec{v},\vec{h})} \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) }{\sum_\vec{h} e^{-E(\vec{v},\vec{h})}}
            - \frac{\sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})} \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) }{\sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})}} \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \Big\langle \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) \Big\rangle_{p(\vec{h}|\vec{v})}
        - \Big\langle \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) \Big\rangle_{p(\vec{v},\vec{h})}
\end{align}
We will use \( \langle \ \cdot \ \rangle_{\text{data}} \) to denote the expectation value w.r.t.\ the data, and \( \langle \ \cdot \ \rangle_{\text{model}} \) to denote the expectation value w.r.t.\ the model.
This gives us
\begin{align}
    \partial_{w_{ij}} \ell(\theta)
        &= \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}} \\
    \partial_{a_i} \ell(\theta)
        &= \langle v_i \rangle_{\text{data}} - \langle v_i \rangle_{\text{model}} \\
    \partial_{b_j} \ell(\theta)
        &= \langle h_j \rangle_{\text{data}} - \langle h_j \rangle_{\text{model}}
\end{align}

\section{QBM}
\subsection{Log-Likelihood}\label{app:qbm_log_likelihood_derivation}
We start with the data-averaged log-likelihood
\begin{align}
    \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log p(\vec{v}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \frac{\tr(\Lambda_\vec{v} e^{-H})}{\tr(e^{-H})} \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \Big[ \log\tr(\Lambda_\vec{v} e^{-H}) - \log\tr(e^{-H}) \Big]
\end{align}
Taking the partial derivative yields
\begin{align}
    \label{eq:qbm_log_likelihood_derivative}
    \partial_\theta \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(\Lambda_\vec{v} \partial_\theta e^{-H})}{\tr(\Lambda_\vec{v} e^{-H})} - \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})} \bigg]
\end{align}
Due to the noncommutativity of \( H \) and \( \partial_\theta H \), we need to use the trick laid out in~\cite{amin_2018} where we take \( e^{-H} = (e^{-\delta\tau H})^n \) with \( \delta\tau \equiv 1 / n \), which allows one to write
\begin{align}
    \partial_\theta e^{-H}
        &= -\sum_{m=1}^{n} e^{-m\delta\tau H} \delta\tau \partial_\theta He^{-(n-m)\delta\tau H} + \mathcal{O}(\delta\tau^2)
\end{align}
Taking the limit as \( n \rightarrow \infty \) of both sides gives
\begin{align}
    \partial_\theta e^{-H}
        &= \lim_{n\rightarrow\infty} -\sum_{m=1}^{n} e^{-m\delta\tau H} \delta\tau \partial_\theta He^{-(n-m)\delta\tau H} + \mathcal{O}(\delta\tau^2) \\
        &= -\int_{0}^{1} d\tau e^{-\tau H} \partial_\theta H e^{(\tau-1)H}
\end{align}
From here one can take the trace of both sides to arrive at
\begin{align}
    \tr(\partial_\theta e^{-H})
        &= -\tr\bigg( \int_{0}^{1} d\tau e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \bigg) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \big) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{(\tau-1)H} e^{-\tau H} \partial_\theta H \big) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{-H} \partial_\theta H \big) \\
        &= -\tr\big(e^{-H} \partial_\theta H \big)
\end{align}
Which gives
\begin{align}
    \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})}
        &= -\frac{\tr(e^{-H} \partial_\theta H)}{\tr(e^{-H})} \\
        &= -\tr(\rho \partial_\theta H) \\
        &= -\langle \partial_\theta H \rangle
\end{align}
Unfortunately, due to the additional factor of \( \Lambda_\vec{v} \) in the first term of \cref{eq:qbm_log_likelihood_derivative}, one arrives at
\begin{align}
    \tr(\Lambda_\vec{v} \partial_\theta e^{-H})
        &= -\tr\bigg( \int_{0}^{1} d\tau \Lambda_\vec{v} e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \bigg) \\
        &= -\int_{0}^{1} d\tau \tr\big(\Lambda_\vec{v} e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \big)
\end{align}
which is nontrivial to compute in practice.

\subsection{Log-Likelihood Lower Bound}\label{app:qbm_log_likelihood_lower_bound}
The Golden-Thompson inequality that \( \tr(e^{A}e^{B}) \ge \tr(e^{A+B}) \) allows one to write (for small \( \epsilon > 0 \))
\begin{align}
    \tr(e^{-H} e^{\log(\Lambda_\vec{v}+\epsilon)}) \ge \tr(e^{-H+\log(\Lambda_\vec{v}+\epsilon)})
\end{align}
Taking the limit \( \epsilon \rightarrow 0 \) yield
\begin{align}
    \lim_{\epsilon\rightarrow 0} \tr(e^{-H+\log(\Lambda_\vec{v}+\epsilon)})
        &= \tr(e^{-H_\vec{v}})
\end{align}
where
\begin{align}
    H_\vec{v} &= \braket{\vec{v} | H | \vec{v}}
\end{align}
with \( H_\vec{v} \) being the "clamped" Hamiltonian.
Using this we can write the inequality
\begin{align}
    p(\vec{v})
        &= \frac{\tr(\Lambda_\vec{v} e^{-H})}{\tr(e^{-H})} \\
        &\ge \frac{\tr(e^{-H_\vec{v}})}{\tr(e^{-H})}
\end{align}
which in turn allows for the log-likelihood to be bounded as
\begin{align}
    \ell(\theta) \ge \tilde{\ell}(\theta)
\end{align}
where
\begin{align}
    \tilde{\ell}(\theta)
        &\equiv \sum_\vec{v} p_\text{data}(\vec{v}) \log\frac{\tr(e^{-H_\vec{v}})}{\tr(e^{-H})}
\end{align}
\subsection{Log-Likelihood Lower Bound Derivative}\label{app:qbm_log_likelihood_lower_bound_derivative}
Taking the partial derivative yields
\begin{align}
    \label{eq:qbm_log_likelihood_derivative_lower_bound}
    \partial_\theta \tilde{\ell}(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(\partial_\theta e^{-H_\vec{v}})}{\tr(e^{-H_\vec{v}})} - \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})} \bigg] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(e^{-H_\vec{v}} \partial_\theta H_\vec{v})}{\tr(e^{-H_\vec{v}})} - \frac{\tr(e^{-H} \partial_\theta H)}{\tr(e^{-H})} \bigg] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) [ \tr(\rho_\vec{v} \partial_\theta H_\vec{v}) - \tr(\rho \partial_\theta H) ] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) [ \langle \partial_\theta H_\vec{v} \rangle_\vec{v} - \langle \partial_\theta H \rangle ] \\
        &= \overline{\langle \partial_\theta H_\vec{v} \rangle_\vec{v}} - \langle \partial_\theta H \rangle
\end{align}
Plugging in our parameters we get (note that we're still speaking in the quantum language with \( -1 \) and \( +1 \), so if we would rather have the results in \( 0 \) and \( 1 \) then we need to subtract add \( 1 \) and divide by \( 2 \) to convert)
\begin{align}
    \partial_{w_{ij}} \tilde{\ell}(\theta)
        &= \overline{\langle \sigma_i^z \sigma_j^z \rangle_\vec{v}} - \langle \sigma_i^z \sigma_j^z \rangle \\
    \partial_{a_i} \tilde{\ell}(\theta)
        &= \overline{\langle \sigma_i^z \rangle_\vec{v}} - \langle \sigma_i^z \rangle \\
    \partial_{b_j} \tilde{\ell}(\theta)
        &= \overline{\langle \sigma_j^z \rangle_\vec{v}} - \langle \sigma_j^z \rangle
\end{align}
When restrictions are imposed on connections within the hidden layer, the clamped Hamiltonian becomes
\begin{align}
    H_\vec{v}
        &= -\sum_i a_i \tilde{v}_i -\sum_j \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big)
\end{align}
where \( b_j'(\tilde{\vec{v}}) = b_j + (W^\intercal\tilde{\vec{v}})_j \).
This allows one to rewrite the clamped density matrix as
\begin{align}
    \rho_\vec{v}
        &= \frac{1}{Z} \exp\bigg( \sum_i a_i \tilde{v}_i + \sum_j \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big) \bigg) \\
        &= \frac{1}{Z} \exp\bigg( \sum_i a_i \tilde{v}_i \bigg) \prod_j \exp \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big)
\end{align}
Therefore, we can see that the clamped density matrix is proportional to the product of the exponentials of the hidden unit factors.
This allows us to compute the expectation values values
\begin{align}
    \langle \sigma_j^z \rangle_\vec{v}
        &= \tr(\rho_\vec{v}\sigma_j^z) \\
        &= \frac{\tr\bigg[ \exp\bigg( \sum_i a_i \tilde{v}_i \bigg) \exp \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big) \sigma_j^z \bigg]}{\tr\bigg[ \exp\bigg( \sum_i a_i \tilde{v}_i \bigg) \exp \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big) \bigg) \bigg]} \\
        &= \frac{\tr\bigg[ \exp \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big) \sigma_j^z \bigg]}{\tr\bigg[ \exp \big(\Gamma_j \sigma_j^x + b_j'(\tilde{\vec{v}}) \sigma_j^z\big) \bigg) \bigg]} \\
        &= \frac{b_j'(\tilde{\vec{v}})}{D_j(\tilde{\vec{v}})} \tanh(D_j) \\
    \langle \sigma_i^z \rangle_\vec{v}
        &= \tilde{v}_i \\
    \langle \sigma_i^z \sigma_j^z \rangle_{\vec{v}}
        &= \tilde{v}_i \frac{b_j'(\tilde{\vec{v}})}{D_j(\tilde{\vec{v}})} \tanh(D_j)
\end{align}
where \( D_j(\tilde{\vec{v}}) = \sqrt{\Gamma_j^2 + \big(b_j'(\tilde{\vec{v}})\big)^2} \).
