\section{Restricted Boltzmann Machine}
\subsection{Conditional Probabilities}\label{app:conditional_probabilities_derivation}
This derivation follows along the lines of that found on p. 658-659 of~\cite{goodfellow_deep_learning}.
We start by noting
\begin{align}
    p(\vec{v}) = \frac{1}{Z} \sum_\vec{h} e^{-E(\vec{v},\vec{h})}
\end{align}
From this we can derive the conditional probability
\begin{align}
\begin{split}
    p(\vec{h} | \vec{v})
        &= \frac{p(\vec{v},\vec{h})}{p(\vec{v})} \\
        &= \frac{1}{p(\vec{v})} \frac{1}{Z} \exp( \vec{a}^\intercal\vec{v} + \vec{b}^\intercal\vec{h} + \vec{v}^\intercal\mat{W}\vec{h} ) \\
        &= \frac{1}{Z'} \exp\bigg( \sum_j b_j h_j + \sum_j (\vec{v}^\intercal\mat{W})_j h_j \bigg) \\
        &= \frac{1}{Z'} \prod_j \exp\big( b_j h_j + (\vec{v}^\intercal\mat{W})_j h_j \big)
\end{split}
\end{align}
where
\begin{align}
    Z' = \sum_\vec{h} \exp( \vec{b}^\intercal\vec{h} + \vec{v}^\intercal\mat{W}\vec{h} )
\end{align}
This leads us to
\begin{align}
\begin{split}
    p(h_j = 1 | \vec{v})
        &= \frac{\tilde{p}(h_j = 1 | \vec{v})}{\tilde{p}(h_j = 0 | \vec{v}) + \tilde{p}(h_j = 1 | \vec{v})} \\
        &= \frac{\exp\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)}{1 + \exp\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)} \\
        &= \sigma\big( b_j + (\vec{v}^\intercal\mat{W})_j \big)
\end{split}
\end{align}
Finally, we have
\begin{align}
    p(\vec{h} | \vec{v}) = \prod_j \sigma\big( (2\vec{h} - 1) \odot (\vec{b} + \mat{W}^\intercal\vec{v}) \big)_j
\end{align}

Analogously for \( p(\vec{v} | \vec{h}) \) one finds
\begin{align}
    p(\vec{v} | \vec{h}) = \prod_i \sigma\big( (2\vec{v} - 1) \odot (\vec{a} + \mat{W}\vec{h}) \big)_i
\end{align}

\subsection{Log-Likelihood}\label{app:rbm_log_likelihood_derivation}
For data distribution \( p_\text{data} \) and parameters \( \theta = (\mat{W}, \vec{a}, \vec{b}) \) the average log-likelihood is given by
\begin{align}
\begin{split}
    \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log p(\vec{v}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \sum_\vec{h} p(\vec{v},\vec{h}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \bigg(\frac{1}{Z} \sum_\vec{h} e^{-E(\vec{v},\vec{h})}\bigg) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \sum_\vec{h} e^{-E(\vec{v},\vec{h})} - \log \sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})}
\end{split}
\end{align}
Taking the gradient we find
\begin{align}
\begin{split}
    \partial_{\theta} \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \frac{\sum_\vec{h} e^{-E(\vec{v},\vec{h})} \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) }{\sum_\vec{h} e^{-E(\vec{v},\vec{h})}}
            - \frac{\sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})} \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) }{\sum_{\vec{v},\vec{h}} e^{-E(\vec{v},\vec{h})}} \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \Big\langle \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) \Big\rangle_{p(\vec{h}|\vec{v})}
        - \Big\langle \partial_{\theta}\big( -E(\vec{v},\vec{h}) \big) \Big\rangle_{p(\vec{v},\vec{h})}
\end{split}
\end{align}
We will use \( \langle \ \cdot \ \rangle_{\text{data}} \) to denote the expectation value w.r.t.\ the data, and \( \langle \ \cdot \ \rangle_{\text{model}} \) to denote the expectation value w.r.t.\ the model.
This gives us
\begin{align}
\begin{split}
    \partial_{w_{ij}} \ell(\theta)
        &= \langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{model}} \\
    \partial_{a_i} \ell(\theta)
        &= \langle v_i \rangle_{\text{data}} - \langle v_i \rangle_{\text{model}} \\
    \partial_{b_j} \ell(\theta)
        &= \langle h_j \rangle_{\text{data}} - \langle h_j \rangle_{\text{model}}
\end{split}
\end{align}

\section{Quantum Boltzmann Machine}
\subsection{Log-Likelihood}\label{app:qbm_log_likelihood_derivation}
This derivation follows along the lines of that laid out in~\cite{amin_2018}.
We start with the data-averaged log-likelihood
\begin{align}
\begin{split}
    \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log p(\vec{v}) \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \log \frac{\tr(\Lambda_\vec{v} e^{-H})}{\tr(e^{-H})} \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \Big[ \log\tr(\Lambda_\vec{v} e^{-H}) - \log\tr(e^{-H}) \Big]
\end{split}
\end{align}
Taking the partial derivative yields
\begin{align}
    \label{eq:qbm_log_likelihood_derivative}
    \partial_\theta \ell(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(\Lambda_\vec{v} \partial_\theta e^{-H})}{\tr(\Lambda_\vec{v} e^{-H})} - \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})} \bigg]
\end{align}
Due to the noncommutativity of \( H \) and \( \partial_\theta H \), we need to use the trick laid out in~\cite{amin_2018} where we take \( e^{-H} = (e^{-\delta\tau H})^n \) with \( \delta\tau \equiv 1 / n \), which allows one to write
\begin{align}
    \partial_\theta e^{-H}
        &= -\sum_{m=1}^{n} e^{-m\delta\tau H} \delta\tau \partial_\theta He^{-(n-m)\delta\tau H} + \mathcal{O}(\delta\tau^2)
\end{align}
Taking the limit as \( n \rightarrow \infty \) of both sides gives
\begin{align}
\begin{split}
    \partial_\theta e^{-H}
        &= \lim_{n\rightarrow\infty} -\sum_{m=1}^{n} e^{-m\delta\tau H} \delta\tau \partial_\theta He^{-(n-m)\delta\tau H} + \mathcal{O}(\delta\tau^2) \\
        &= -\int_{0}^{1} d\tau e^{-\tau H} \partial_\theta H e^{(\tau-1)H}
\end{split}
\end{align}
From here one can take the trace of both sides to arrive at
\begin{align}
\begin{split}
    \tr(\partial_\theta e^{-H})
        &= -\tr\bigg( \int_{0}^{1} d\tau e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \bigg) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \big) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{(\tau-1)H} e^{-\tau H} \partial_\theta H \big) \\
        &= -\int_{0}^{1} d\tau \tr\big(e^{-H} \partial_\theta H \big) \\
        &= -\tr\big(e^{-H} \partial_\theta H \big)
\end{split}
\end{align}
Which gives
\begin{align}
\begin{split}
    \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})}
        &= -\frac{\tr(e^{-H} \partial_\theta H)}{\tr(e^{-H})} \\
        &= -\tr(\rho \partial_\theta H) \\
        &= -\langle \partial_\theta H \rangle
\end{split}
\end{align}
Unfortunately, due to the additional factor of \( \Lambda_\vec{v} \) in the first term of \cref{eq:qbm_log_likelihood_derivative}, one arrives at
\begin{align}
\begin{split}
    \tr(\Lambda_\vec{v} \partial_\theta e^{-H})
        &= -\tr\bigg( \int_{0}^{1} d\tau \Lambda_\vec{v} e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \bigg) \\
        &= -\int_{0}^{1} d\tau \tr\big(\Lambda_\vec{v} e^{-\tau H} \partial_\theta H e^{(\tau-1)H} \big)
\end{split}
\end{align}
which is nontrivial to compute in practice.

\subsection{Log-Likelihood Lower Bound}\label{app:qbm_log_likelihood_lower_bound}
This derivation follows along the lines of that laid out in~\cite{amin_2018}.
The Golden-Thompson inequality that \( \tr(e^{A}e^{B}) \ge \tr(e^{A+B}) \) allows one to write (for small \( \epsilon > 0 \))
\begin{align}
    \tr(e^{-H} e^{\log(\Lambda_\vec{v}+\epsilon)}) \ge \tr(e^{-H+\log(\Lambda_\vec{v}+\epsilon)})
\end{align}
Taking the limit \( \epsilon \rightarrow 0 \) yields
\begin{align}
    \tr(\Lambda_\vec{v}e^{-H}) \ge \tr(e^{-H_\vec{v}})
\end{align}
where
\begin{align}
    H_\vec{v} &= \braket{\vec{v} | H | \vec{v}}
\end{align}
with \( H_\vec{v} \) being the "clamped" Hamiltonian.
This is called clamped because the visible qubits are held to the classical state of the visible vector \( \vec{v} \) due to an infinite energy penalty imposed by the \( \log(\Lambda_\vec{v} + \epsilon) \) term.
Using this we can write the inequality
\begin{align}
\begin{split}
    p(\vec{v})
        &= \frac{\tr(\Lambda_\vec{v} e^{-H})}{\tr(e^{-H})} \\
        &\ge \frac{\tr(e^{-H_\vec{v}})}{\tr(e^{-H})}
\end{split}
\end{align}
which in turn allows for the log-likelihood to be bounded as
\begin{align}
    \ell(\theta) \ge \tilde{\ell}(\theta)
\end{align}
where
\begin{align}
    \tilde{\ell}(\theta)
        &\equiv \sum_\vec{v} p_\text{data}(\vec{v}) \log\frac{\tr(e^{-H_\vec{v}})}{\tr(e^{-H})}
\end{align}
\subsection{Log-Likelihood Lower Bound Derivative}\label{app:qbm_log_likelihood_lower_bound_derivative}
This derivation follows along the lines of that laid out in~\cite{amin_2018}.
Taking the partial derivative yields
\begin{align}
\begin{split}
    \label{eq:qbm_log_likelihood_derivative_lower_bound}
    \partial_\theta \tilde{\ell}(\theta)
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(\partial_\theta e^{-H_\vec{v}})}{\tr(e^{-H_\vec{v}})} - \frac{\tr(\partial_\theta e^{-H})}{\tr(e^{-H})} \bigg] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) \bigg[ \frac{\tr(e^{-H_\vec{v}} \partial_\theta H_\vec{v})}{\tr(e^{-H_\vec{v}})} - \frac{\tr(e^{-H} \partial_\theta H)}{\tr(e^{-H})} \bigg] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) [ \tr(\rho_\vec{v} \partial_\theta H_\vec{v}) - \tr(\rho \partial_\theta H) ] \\
        &= \sum_{\vec{v}} p_{\text{data}}(\vec{v}) [ \langle \partial_\theta H_\vec{v} \rangle_\vec{v} - \langle \partial_\theta H \rangle ] \\
        &= \overline{\langle \partial_\theta H_\vec{v} \rangle_\vec{v}} - \langle \partial_\theta H \rangle
\end{split}
\end{align}
Plugging in our parameters we get
\begin{align}
\begin{split}
    \partial_{w_{ij}} \tilde{\ell}(\theta)
        &= \overline{\langle \sigma_i^z \sigma_j^z \rangle_\vec{v}} - \langle \sigma_i^z \sigma_j^z \rangle \\
        &= \langle \sigma_i^z \sigma_j^z \rangle_\text{data} - \langle \sigma_i^z \sigma_j^z \rangle_\text{model} \\
    \partial_{b_i} \tilde{\ell}(\theta)
        &= \overline{\langle \sigma_i^z \rangle_\vec{v}} - \langle \sigma_i^z \rangle \\
        &= \langle \sigma_i^z \rangle_\text{data} - \langle \sigma_i^z \rangle_\text{model}
\end{split}
\end{align}
When restrictions are imposed on connections within the hidden layer, the clamped Hamiltonian reduces to
\begin{align}
    H_\vec{v}
        &= -\sum_i \big(\Gamma_i \sigma_i^x + b_i'(\vec{v}) \sigma_i^z\big)
\end{align}
where \( b_i'(\vec{v}) = b_i + (\mat{W}^\intercal\vec{v})_i \).
This allows one to rewrite the clamped density matrix as
\begin{align}
\begin{split}
    \rho_\vec{v}
        &= \frac{1}{Z_\vec{v}} \exp\bigg( \sum_i \big(\Gamma_i \sigma_i^x + h_i'(\vec{v}) \sigma_i^z\big) \bigg) \\
        &= \frac{1}{Z_\vec{v}} \prod_i \exp \big(\Gamma_i \sigma_i^x + b_i'(\vec{v}) \sigma_i^z\big) \\
        &= \prod_i \rho_\vec{v}^{(i)}
\end{split}
\end{align}
With this we can compute the expectation values as
\begin{align}
\begin{split}
    \langle \sigma_i^z \rangle_\vec{v}
        &= \tr(\rho_\vec{v}^{(i)}\sigma_i^z) \\
        &= \frac{\tr\bigg[ \exp \big(\Gamma_i \sigma_i^x + b_i'(\vec{v}) \sigma_i^z\big) \sigma_i^z \bigg]}{\tr\bigg[ \exp \big(\Gamma_i \sigma_i^x + b_i'(\vec{v}) \sigma_i^z\big) \bigg]} \\
        &= \frac{b_i'(\vec{v})}{D_i(\vec{v})} \tanh\big(D_i(\vec{v})\big)
\end{split}
\end{align}
where \( D_i(\vec{v}) = \sqrt{\Gamma_i^2 + b_i'(\vec{v})^2} \).

The last equality above is obtained by using that for traceless \( A \) with \( \det A < 0 \) we can write
\begin{align}
    \exp(A) = \cosh\Big(\sqrt{\abs{\det A}}\Big) I + \frac{1}{\sqrt{\abs{\det A}}}\sinh\Big(\sqrt{\abs{\det A}}\Big) A
\end{align}
This is obtained by using Cayley-Hamilton theorem along with the series expansion of the matrix exponential and grouping the terms.

\subsection{Learning The Effective Inverse Temperature \( \beta \)}\label{app:learning_beta}
Following along the lines of~\cite{xu_2021} we have
\begin{align}
\begin{split}
    p_\text{DW}
        &= \frac{1}{Z_\text{DW}} e^{-E_\text{DW}} \\
        &= \frac{1}{Z_\text{DW}} e^{-E / \betahat} \\
\end{split}
\end{align}
which leads to a negative log-likelihood derivative of
\begin{align}
    -\frac{\partial \log p_\text{DW}}{\partial\betahat}
        &= -\frac{1}{\betahat^2} (E - \langle E \rangle)
\end{align}
and after averaging over the training data configurations we have
\begin{align}
    \Delta\betahat
        &= \frac{\eta}{\betahat^2}\big(\langle E \rangle_\text{data} - \langle E \rangle_\text{model}\big)
\end{align}

\section{Model Implementation in Python}
A Python package named \texttt{qbm} was developed to house all of the reusable code for this project.
The code implementing the quantum restricted Boltzmann machine, trained by minimizing the upper bound of the negative log-likelihood, is implemented as an easy to use Python class titled \texttt{BQRBM}.
All code is commented and broken down into easy to understand methods and blocks.

\subsection{Initialization}
The object can be initialized as follows
\begin{python}
model = BQRBM(
    X_train,
    n_hidden,
    embedding,
    anneal_params,
    beta_initial=1.5,
    beta_range=(0.1, 10),
    qpu_params={"region": "na-west-1", "solver": "Advantage_system4.1"},
    exact_params=None,
    seed=0,
)
\end{python}
where
\begin{itemize}
    \item \texttt{X\_train (np.ndarray)}: training data set of visible vectors, of shape \newline\texttt{(n\_training\_examples, n\_visible)}.
    \item \texttt{n\_hidden (int)}: number of hidden units.
    \item \texttt{embedding (dict)}: embedding dict (can be generated by \texttt{minorminer}).
    \item \texttt{anneal\_params (dict)}: dict containing the keys \texttt{s} corresponding to \( s^* \), \texttt{A} corresponding to \( A(s^*) \), and \texttt{B} corresponding to \( B(s^*) \) that determine which distribution the model will use to approximate with, and \texttt{anneal\_schedule} which is a list of tuples of the form \( (t, s) \) defining the anneal schedule.
    \item \texttt{beta\_initial (float)}: the initial value of \( \betahat \) to approximate the effective \( \beta \). It is recommended to tune this accordingly. One method is to train a model for a couple of epochs, see where the value settles around, then set to that on the next initialization.
    \item \texttt{beta\_range (2-tuple of floats)}: range of allowed values for \( \betahat \). Used for clipping \( \betahat \) to sane values, e.g. to avoid accidentally overshooting and learning a negative value.
    \item \texttt{qpu\_params (dict)}: parameters dict to initialize the qpu.
    \item \texttt{exact\_params (dict)}: optional dict with keys \texttt{beta} corresponding to the effective \( \beta \) the exact sampler will use. If not \texttt{None} will result in the model using the exact sampler rather than the annealer.
    \item \texttt{seed (int)}: seed for the (pseudo) random number generator used in shuffling the mini-batches, as well as the exact sampler when used.
\end{itemize}

\subsection{Training}
The model can be trained using the \texttt{train()} method
\begin{python}
model.train(
    n_epochs=100,
    learning_rate=1e-1,
    learning_rate_beta=1e-1,
    mini_batch_size=10,
    n_samples=10_000,
    callback=None,
)
\end{python}
where
\begin{itemize}
    \item \texttt{n\_epochs (int)}: the number of epochs to train.
    \item \texttt{learning\_rate (float or list of floats)}: the learning rate to use when updating the weights and biases. If a \texttt{list} then must be of length \texttt{n\_epochs}, where \texttt{learning\_rate[i-1]} is the learning rate to use at epoch \texttt{i}. It is highly recommended to tune this accordingly.
    \item \texttt{learning\_rate\_beta (float or list of floats)}: the learning rate to use when updating \( \betahat \). If a \texttt{list} then must be of length \texttt{n\_epochs}, where \newline\texttt{learning\_rate\_beta[i-1]} is the learning rate to use during epoch \texttt{i}. It is highly recommended to tune this accordingly.
    \item \texttt{mini\_batch\_size (int)}: size of the mini-batches. It is recommended to keep this around 10, similar to a classical RBM.
    \item \texttt{n\_samples (int)}: number of samples to generate at the end of every epoch to use for updating \( \betahat \), as well as in the \texttt{callback()} function. It is recommended to keep this as \( 10^4 \) when using the annealer to get the best read on the statistics as possible.
    \item \texttt{callback (function)}: function which takes the arguments \texttt{(model, sample\_state\_vectors)} to call at the end of every epoch for the purpose of getting a read on the model's training progress. The \texttt{model} argument is the BQRBM object, and the \texttt{sample\_state\_vectors} are the sampled state vectors returned by the sampler, of shape \texttt{(n\_samples, n\_qubits)} with the first \texttt{n\_visible} qubits corresponding to the visible units. If \texttt{None}, then no callback function will be used.
\end{itemize}

\subsubsection{Training Loop}
Below we have a code snippet from the core of the \texttt{train} method
\begin{python}[breaklines=true]
for epoch in range(1, n_epochs + 1):
    # loop over the mini-batches and compute and apply grads for each
    for mini_batch_indices in self._random_mini_batch_indices(mini_batch_size):
        V_pos = self.X_train[mini_batch_indices]
        self._compute_positive_grads(V_pos)
        self._compute_negative_grads(V_pos.shape[0])
        self._apply_grads(self.learning_rate / V_pos.shape[0])
        self._clip_weights_and_biases()

    # generate samples after every epoch to update beta
    samples = self.sample(n_samples)
    self._update_beta(samples)

    # pass samples to the callback function to track performance
    if callback is not None:
        callback_output = callback(self, self._get_state_vectors(samples))
        self.callback_outputs.append(callback_output)
\end{python}
In the above training loop we see the \texttt{\_clip\_weights\_and\_biases()} method being called.
This method clips the weights and biases so that the corresponding \( h_i \) and \( J_{ij} \) values fall within the allowed ranges for the annealer, which for the Advantage 4.1 is \( h_i \in [-4, 4] \) and \( J_{ij} \in [-1, 1] \)~\cite{dwave_solver_properties}.
In principle, it is undesirable to have the weights and biases ever be clipped as it alters the state of the problem.
Therefore, this is just here so that the annealer only ever receives valid \( h_i \) and \( J_{ij} \) values.

\subsection{Sampling}
After the model is trained it can be sampled via the \texttt{sample()} method
\begin{python}
model.sample(
    n_samples,
    answer_mode="raw",
    use_gauge=True,
    binary=False,
)
\end{python}
where
\begin{itemize}
    \item \texttt{n\_samples (int)}: size of the sample set to be generated. The D-Wave Advantage 4.1 annealer is limited to a maximum of \( 10^4 \).
    \item \texttt{answer\_mode (str)}: either \texttt{"raw"} or \texttt{"histogram"}, see the corresponding D-Wave documentation~\cite{dwave_solver_parameters} for more information.
    \item \texttt{use\_gauge (bool)}: if \texttt{True} will use a random gauge transformation, the results will be automatically transformed back to the original space. This is usually recommended to mitigate any readout biases the QPU might have.
    \item \texttt{binary (bool)}: if \texttt{True} will return the sampled state vectors in binary values rather than spin eigenvalues.
\end{itemize}

\subsection{Saving and Loading}
Models can easily be saved and loaded via the corresponding \texttt{save} and \texttt{load} methods.
A model can be saved using
\begin{python}
model.save(file_path)
\end{python}
and loaded again using
\begin{python}
model = BQRBM.load(file_path)
\end{python}

\section{Miscellaneous}
\subsection{Annualized Volatility}\label{app:annualized_volatility}
In finance, the annualized volatility of a time series vector \( \vec{x} \) is computed as
\begin{align}
    \text{vol}(\vec{x}) = \sqrt{252} \cdot \text{std}(\vec{x})
\end{align}
where the factor of \( \sqrt{252} \) comes from the square root of the number of trading days in a year, i.e., it's the annualization factor.

\subsection{KL Divergence}\label{app:kl_divergence}
The Kullback-Leibler divergence~\cite{kullback_1951} is measure of how much the probability distribution \( q \) differs from the reference probability distribution \( p \).
It is defined as
\begin{align}
    \DKL{p}{q}
        &= \sum_{x} p(x) \log\frac{p(x)}{q(x)}
\end{align}
It can be interpreted as the amount of information loss associated with using \( q \) to approximate \( p \).
It must also be noted that the KL divergence is a distance, but not a metric (rather a divergence), because of the asymmetry that \( \DKL{p}{q} \ne \DKL{q}{p} \).

\subsubsection{KL Divergence in Practice}\label{app:kl_divergence_in_practice}
If we wish to compare the distribution \( \psamples \) obtained from real world sampling to that of the reference distribution \( \ptheory \) computed exactly, we need to first do some preprocessing to ensure that our comparison carries some sort of meaning.
Since \( \psamples \) is derived from a finite number of observations we will not always be able to get a full read of the true distribution.
Therefore, it is ideal to take a histogram approach to better approximate the shape of the distribution and compare it to the same binned shape of the reference distribution.
In practice this depends on how many samples ones has, and how many samples there are in comparison to the total number of states of the distribution.

For the problem of comparing the distribution obtained by sampling the D-Wave annealer (maximum \( 10^4 \) sample size) in relation to \( 2^{12} = 4096 \) total possible configurations, we chose the number of bins in the histogram to be 32, since this is close to the number of bins computed using the Freedman-Diaconis rule on some of the sample sets.

An issue we run into in practice is that \( \psamples \) computed via the histogram approach might contain zeros, which leads to issues when computing the KL divergence since it would diverge to \( \infty \) due to the zero in the denominator of the log argument of
\begin{align}
    \DKL{\ptheory}{\psamples} = \sum_i \ptheory^{(i)} \log\frac{\ptheory^{(i)}}{\psamples^{(i)}}
\end{align}
The way around this is the concept of smoothing~\cite{han_kl_divergence}, i.e., adding a small probability to the zeros in the case where we know the true probability to be nonzero.
In this case, given the quantum nature of the problem and the distribution we know that no state has a truly zero probability (although they can be infinitesimally small).
The real question is what probability do we add here so that the numbers still carry some meaning.
For this we chose to take the following approach: if \( \psamples^{(i)} = 0 \) and \( \ptheory^{(i)} > 0 \), then we take \( \psamples^{(i)} = \ptheory^{(i)} \cdot 10^{-6} \), which means that the term \( \ptheory^{(i)} \log \frac{\ptheory^{(i)}}{\psamples^{(i)}} = \ptheory^{(i)} \log 10^6 \).
Thus, any zeros in the sample distribution that are nonzero in the exact distribution will impose a \( \ptheory \) scaled constant penalty to the KL divergence.
The sum of the terms of the sample probabilities that used to be zero are then evenly subtracted from the rest of the probabilities to ensure that the distribution remains normalized.

Even though this is not the true KL divergence as intended by theory, it provides a decent approximation for which we can use to compare distributions generated by real world processes with limited sample size.


\subsection{Correlation Coefficients}\label{app:correlation_coefficients}
The Pearson correlation coefficient is defined as
\begin{align}
    \rho_{X,Y} = \frac{\text{cov}{(X,Y)}}{\sigma_X \sigma_Y} \in [-1, 1]
\end{align}
and measures the linear correlation between the random variables \( X \) and \( Y \).
Therefore, it must be noted that this does not capture nonlinear relations, thus it shouldn't be relied upon to tell the full story.
Additionally, this measure is quite sensitive to outliers.

The Spearman rank correlation coefficient is defined as
\begin{align}
    r_s = \rho_{R(X),R(Y)} = \frac{\text{cov}{\big(R(X),R(Y)\big)}}{\sigma_{R(X)} \sigma_{R(Y)}} \in [-1, 1]
\end{align}
and is the Pearson correlation coefficient of the rank of the random variables \( X \) and \( Y \).
The main difference to the Pearson correlation coefficient is that the Spearman measures the monotonic relationship, regardless of linearity.
The Spearman correlation coefficient is also less sensitive to outliers than the Pearson.

The Kendall rank correlation coefficient is defined as
\begin{align}
    \tau = \frac{2}{n(n-1)} \sum_{i<j} \text{sign}(x_i - x_j) \text{sign}(y_i - y_j) \in [-1, 1]
\end{align}
where \( (x_1, y_1), \dots, (x_n, y_n) \) are pairs of observations of the random variables \( X \) and \( Y \).

It is important to keep in mind how one interprets the correlation coefficients.
The sign of the correlation coefficient determines whether or not it is negatively or positively correlated, and the magnitude determines how strong the correlation effects are.
As a loose guide, correlation coefficient values of 0.1, 0.3, and 0.5 can be termed small, medium, and large, respectively~\cite{research_design_and_statistical_analysis}.
In general, one must be careful when interpreting the correlation coefficients, thus it is important to understand what the values mean, and what they don't.
Section 3.4.2 "Interpreting the Correlation Coefficient" of~\cite{research_design_and_statistical_analysis} offers further insight and points out some of the pitfalls to watch out for.

In this thesis the correlation coefficients are computed using the respective functions from the SciPy Python package~\cite{python_scipy}.

\subsection{Tail Concentration Functions}\label{app:tail_concentration_functions}
The lower tail concentration function is defined as~\cite{venter_2002}
\begin{align}
\begin{split}
    L(z)
        &= \frac{p(U_1 \le z, U_2 \le z)}{z} \\
        &= \frac{C(z,z)}{z}
\end{split}
\end{align}
and the upper as
\begin{align}
\begin{split}
    R(z)
        &= \frac{p(U_1 > z, U_2 > z)}{1-z} \\
        &= \frac{1 - 2z + C(z,z)}{1-z}
\end{split}
\end{align}
where \( U_1 \) and \( U_2 \) uniform random variables on the interval \( [0, 1] \), and \( C(u_1, u_2) \) is the copula of \( (U_1, U_2) \).

In practice we compute \( U_1 \) and \( U_2 \) as the normalized rank of the observations of the random variables \( X \) and \( Y \), respectively.
The way to interpret the concentration functions is that they represent the probability that \( X \) and \( Y \) simultaneously take on extreme values.
When plotted, the lower tail concentration function is used for \( 0 \le z \le 0.5 \) and the upper for \( 0.5 \le z \le 1 \).

\subsection{Autocorrelation Analysis}\label{app:autocorrelation_analysis}
When studying results from an MCMC-based model it is important to be aware that sequentially generated samples are not always statistically independent, that is, there is some thermalization threshold that corresponds to the minimum number of sampling steps between samples to consider them as statistically independent.

For a time series \( x_1, \dots, x_n \), the lag-\( k \) autocorrelation function is defined as~\cite{time_series_analysis}
\begin{align}
    \rho_k
        &= \frac{\text{cov}(x_t, x_{t+k})}{\sigma_{x}^2}
\end{align}
The autocorrelation function is essentially a correlation coefficient, except instead of comparing two different variables it compares the same variable at different times.
In this thesis we use the statsmodels Python package~\cite{python_statsmodels} to compute the autocorrelation function as there are some caveats when computing it in practice with large chains (e.g. there are some tricks such as using a Fourier transformation to make the computations more efficient).

The integrated autocorrelation time is a reasonable estimate of how many steps in between samples we should have before we can consider them to be (to a degree) statistically independent.
In this thesis we use the emcee Python package~\cite{python_emcee} to estimate the integrated autocorrelation time, which follows the approach laid out by Goodman and Weare in~\cite{goodman_weare_2010}.

\subsection{Exact Computation of \( \rho \)}\label{app:exact_rho_computation}
For the density matrix
\begin{align}
    \rho = \frac{1}{Z} e^{-\beta H}
\end{align}
we can compute this as
\begin{align}
    \rho
        &= \frac{1}{\tr(A)} S A S^{-1}
\end{align}
where
\begin{align}
    A = \text{diag}\Big(e^{-\beta(\lambda_1 - \lambda_{\min})}, \dots, e^{-\beta(\lambda_{2^n} - \lambda_{\min})}\Big)
\end{align}
with \( \lambda_i \) being the eigenvalues of \( H \), and \( S \) is the matrix of eigenvectors that transforms \( H \) to and from its eigenspace.
The reason why we subtract \( \lambda_{\min} \) from the eigenvalues in practice is to avoid computing the exponential of a large number which can lead to divergence in floating point calculations.

\subsection{Learning Rate Decay Schedule}\label{app:lr_exp_decay}
The learning rate at epoch \( t \) is given by
\begin{align}
    \eta^{(t)}
        &= \eta^{(0)} \cdot \min\bigg\{1, 2^{-\frac{t - t_\text{decay}}{T_\text{decay}}}\bigg\}
\end{align}
where \( \eta^{(0)} \) is the initial learning rate, \( t_\text{decay} \) is the epoch at which the decay begins, and \( T_\text{decay} \) is the decay period.
