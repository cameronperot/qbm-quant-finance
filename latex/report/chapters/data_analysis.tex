\section{Data Analysis}
Our raw data set consists of the daily open, high, low, and close (OHLC) values for the time period 1999-01-01 through 2019-12-31 of the following major currency pairs
\begin{itemize}
    \item EURUSD - Euro € / U.S. Dollar \$
    \item GBPUSD - British Pound Sterling £ / U.S. Dollar \$
    \item USDCAD - U.S. Dollar \$ / Canadian Dollar \$
    \item USDJPY - U.S. Dollar \$ / Japanese Yen ¥
\end{itemize}
obtained from Dukascopy historical data feed~\cite{dukascopy}.
We filter the data set to remove days with zero volume, as well as NYSE and LSE holidays, resulting in 5165 training samples.
Here we use the notation \( x_\text{open} \), \( x_\text{high} \), \( x_\text{low} \), and \( x_\text{close} \) to denote the open, high, low, and close values of a currency pair on a particular day.

Given that the raw data values are on an absolute basis, we need to convert them to relative terms in order to be able to compare data from different time periods on a more equal footing.
The natural way to do so is to use the intraday returns
\begin{align}
    r = \frac{x_\text{close} - x_\text{open}}{x_\text{open}}.
\end{align}
However, this is not necessarily the best way to approach this.
Instead, we opt to use the log returns
\begin{align}
    \tilde{r}
        = \log(1+r)
        = \log\bigg( \frac{x_\text{close}}{x_\text{open}} \bigg)
\end{align}
due to several advantages, such as log-normality and small \( r \) approximation~\cite{quantivity_2012}.

We begin our analysis by taking a look at the histograms depicted in~\cref{fig:histograms_raw}.
From visual examination we see that the log returns are roughly normally distributed with the statistics given in~\cref{tbl:data_log_returns_raw_stats}.
\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/histograms.png}
    \end{center}
    \caption{Histograms of the log returns data set.}
    \label{fig:histograms_raw}
\end{figure}

\begin{table}[!htb]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \input{../tables/data/log_returns_raw_stats.tbl}
    \end{adjustbox}
    \caption{Statistics of the log returns data set.}
    \label{tbl:data_log_returns_raw_stats}
\end{table}

We also visualize the log returns in a violin and box plot in~\cref{fig:violin_raw} to identify outliers and see how they are distributed.
Two major outliers clearly stand out from the rest: one to the downside for the GBPUSD pair, and another to the upside for the USDJPY pair.
The former occurred on 2016-06-24, the day the Brexit referendum result was announced~\cite{brexit_gov_uk}.
The latter occurred on 2008-10-28, right in the midst of the financial crisis when people were talking about the end of the Yen carry trade~\cite{jpy_carry_trade_nyt}.
In the final training data set, we remove outliers greater than \( 10\sigma \) from the mean, resulting in only removing the day corresponding to the Brexit referendum result, which lies \( 11.1\sigma \) below the mean.
\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/violin.png}
    \end{center}
    \caption{Violin and box plot of the log returns data set illustrating the distribution of the outliers.}
    \label{fig:violin_raw}
\end{figure}

Next we examine the correlations between the currency pairs to get an idea of the interdependencies between them.
We visualize this with scatter plots shown in~\cref{fig:scatters} where we observe a clear positive correlation between EURUSD/GBPUSD, and clear negative correlations between EURUSD/USDCAD and GBPUSD/USDCAD, where the / is used to denote the pairs being compared against each other.
This is further verified by the Pearson \( r \), Spearman \( \rho \), and Kendall \( \tau \) correlation coefficients laid out in~\cref{tbl:data_correlation_coefficients}.
Furthermore, we find the correlation coefficients to be positive for pairs of the form \( X \)USD/\( Y \)USD, and negative for pairs of the form \( X \)USD/USD\( Y \), for \( X,Y \in \) \{EUR, GBP, CAD, JPY\}, as expected.
Details on how the correlation coefficients are computed and how to interpret them can be found in~\cref{app:correlation_coefficients}.
\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/scatters.png}
    \end{center}
    \caption{Scatter plots of the log returns data set.}
    \label{fig:scatters}
\end{figure}

\begin{table}[!htb]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \input{../tables/data/correlation_coefficients.tbl}
    \end{adjustbox}
    \caption{Correlation coefficients of the log returns data set.}
    \label{tbl:data_correlation_coefficients}
\end{table}


\section{Data Preprocessing}
The models in the following chapters require the training data to be in the form of bit vectors, so we must first convert our data set to such a form.
Let \( \mat{X} \in \R^{4 \times N} \) represent the training data set of log returns with \( N \) samples, where training samples are vectors in the column space, thus element \( x_{ij} \) represents the \( i \)th currency pair log return for the \( j \)th training sample.

To discretize the data, we rescale and round the entries of \( \mat{X} \) to integer values in \( \{0, 1, \dots, 2^{n_\text{bits}} - 1\} \), represented by the matrix \( \mat{X}' \in \N^{4 \times N} \) with entries
\begin{align}
    x_{ij}' = \bigg\lfloor \frac{x_{ij} - \min_k \{x_{ik}\}}{\max_k \{x_{ik}\} - \min_k \{x_{ik}\}} \cdot (2^{n_\text{bits}} - 1) \bigg\rceil,
\end{align}
where \( \lfloor \ \cdot \ \rceil \) denotes rounding to the nearest integer.

A new matrix \( \mat{V} \in \binset^{4\cdot n_\text{bits} \times N} \) is then created with the columns being the \( n_\text{bits} \)-length bit vectors corresponding to the binary representation of the entries of the columns of \( \mat{X}' \) concatenated together.
For example, if \( \vec{x}' = (x_1',x_2',x_3',x_4') \) is a column of \( \mat{X}' \) and the function \( \text{bitvector}(x') \) takes in an integer \( x' \) and returns an \( n_\text{bits} \)-bit binary representation bit vector, then the corresponding column in \( \mat{V} \) is
\begin{align}
    \vec{v} = \begin{bmatrix}
        \text{bitvector}(x_1') \\
        \text{bitvector}(x_2') \\
        \text{bitvector}(x_3') \\
        \text{bitvector}(x_4') \\
    \end{bmatrix}
    \in \binset^{4\cdot n_\text{bits}}.
\end{align}

For this research we take \( n_\text{bits} = 16 \), giving us a training set \( \mat{V} \in \binset^{64 \times N} \), thus our training samples are bit vectors of length 64.
The discretization errors associated with this conversion and data set are on the order of \( 10^{-7} \), well within the desired tolerance for this purpose.

\subsection{Data Transformation}\label{sec:outlier_transform}
Due to how the data is linearly converted to a discrete form before rounding, it opens up the possibility of the discretized data being clustered in the mid-range values if large outliers are present.
To mitigate this, we use a transformation to reduce the gap between outliers by scaling outliers beyond a certain threshold \( \tau \) using the procedure detailed in~\cref{alg:transformation}.
We call this the \textit{outlier power transformation}.

In practice, we take \( \tau = 1 \) and \( \alpha = 0.5 \), thus the standardized data points above one standard deviation are mapped to their square roots, as illustrated in~\cref{fig:data_transformation}.
We tested a few other combinations of \( \tau \) and \( \alpha \), but found these values to produce the best model results out of those we tried; of course this could likely be further optimized.
The effect this transformation has on the model results versus the base dataset can be seen in~\cref{sec:rbm_results}.
This transformation is invertible when \( \bar{x} \), \( \sigma_x \), and \( \delta \) are saved.

\begin{algorithm}
\caption{Outlier Power Transformation}
\begin{algorithmic}[1]
    \Procedure{Transform}{$\vec{x}, \alpha, \tau$}
            \Comment $\alpha$ is the power, $\tau$ is the threshold
        \State $N \gets \text{length}(\vec{x})$
        \State $\bar{x} \gets \frac{1}{N} \sum_{i=1}^{N} x_i$
        \State $\sigma_{x} \gets \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2}$
        \State $\delta \gets \tau - \tau^\alpha$
            \Comment ensures the transformation is bijective
        \For {$i$ in 1 to $N$}
            \State $x_i \gets (x_i - \bar{x}) / \sigma_x$
                \Comment standardize
            \If {$x_i > \tau$}
                \State $x_i \gets (\abs{x_i}^\alpha + \delta) \cdot \text{sign}(x_i)$
                    \Comment scale standardized values beyond $\tau$
            \EndIf
            \State $x_i \gets x_i \cdot \sigma_x + \bar{x}$
                \Comment undo standardization
        \EndFor
    \EndProcedure
\end{algorithmic}
\label{alg:transformation}
\end{algorithm}

\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/data_transformation.png}
    \end{center}
    \caption{Transformation defined in~\cref{alg:transformation} using \( \tau = 1 \) and \( \alpha = 0.5 \), for the purpose of reducing large gaps in the discretized data set by scaling outliers above \( \tau \) standard deviations.}
    \label{fig:data_transformation}
\end{figure}

Histograms of the transformed data set are shown in~\cref{fig:histograms_transformed}, and a violin and box plot is shown in~\cref{fig:violin_transformed}.
In these, we observe the appearance of "shoulders" around the threshold \( \tau = 1 \) standard deviation, and that the transformed outliers appear much less extreme, allowing us to better utilize the full range of discrete values.
\cref{tbl:data_log_returns_transformed_stats} shows that the transformation reduces the standard deviations to roughly \( 78\% \) of their originals values given in~\cref{tbl:data_log_returns_raw_stats}.

\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/histograms_transformed.png}
    \end{center}
    \caption{Histograms of the outlier power-transformed log returns data set.}
    \label{fig:histograms_transformed}
\end{figure}
\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/violin_transformed.png}
    \end{center}
    \caption{Violin and box plot of the outlier power-transformed log returns data set illustrating the distribution of the rescaled outliers.}
    \label{fig:violin_transformed}
\end{figure}
\begin{table}[!htb]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \input{../tables/data/log_returns_transformed_stats.tbl}
    \end{adjustbox}
    \caption{Statistics of the outlier power-transformed log returns data set.}
    \label{tbl:data_log_returns_transformed_stats}
\end{table}

\subsection{Additional Information}
As mentioned in~\cite{kondratyev_2019}, one can use additional binary indicator variables to enrich the training data set.
One such bit of information is the rolling volatility relative to the historical median (see~\cref{app:annualized_volatility} for definition of annualized volatility).
If the 3-month rolling volatility is below (above) the historical median it is assigned a value of 0 (1) to indicate the low (high) volatility regime.
The 3-month rolling volatilities versus their historical medians are plotted in~\cref{fig:rolling_volatility}.

These additional binary indicator variables are then concatenated onto the training data set and fed to the model to make it more flexible by allowing for the model outputs to be conditioned on a specific volatility regime.
Adding one indicator for each of the four currency pairs increases the number of rows in our training data set by four, thus the volatility-concatenated data set is in the space \( \binset^{68 \times N} \).

\begin{figure}[!htb]
    \begin{center}
        \includegraphics[width=1\linewidth]{data_analysis/rolling_volatility.png}
    \end{center}
    \caption{3-month rolling volatilities of the log returns data set compared with their historical medians.}
    \label{fig:rolling_volatility}
\end{figure}
